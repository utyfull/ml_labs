{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117073,"databundleVersionId":13976910,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom itertools import combinations\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\ntrain_csv = \"/kaggle/input/mai-ml-lab-1-308/train.csv\"\ntest_csv  = \"/kaggle/input/mai-ml-lab-1-308/test.csv\"\n\nabs_cap = 1000.0\ntop_k = 8\ndegree = 4\nalpha = 6\nclip_val = 10.0\neps = 1e-6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.087525Z","iopub.execute_input":"2025-11-12T08:18:31.087910Z","iopub.status.idle":"2025-11-12T08:18:31.094931Z","shell.execute_reply.started":"2025-11-12T08:18:31.087878Z","shell.execute_reply":"2025-11-12T08:18:31.093650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(train_csv)\n\ntarget_col = \"RiskScore\"\n\ndf[target_col] = pd.to_numeric(df[target_col])\ndf = df.dropna(subset=[target_col])\n\ndf = df[np.isfinite(df[target_col])]\n\nif abs_cap is not None:\n    df = df[df[target_col].abs() <= abs_cap]\n\ncat_cols = [\n    \"MaritalStatus\",\n    \"HomeOwnershipStatus\",\n    \"LoanPurpose\",\n    \"EmploymentStatus\",\n    \"EducationLevel\",\n]\n\nX_numeric = df.select_dtypes(include=[np.number]).drop(columns=[target_col])\nX_categorical = df[cat_cols].copy()\n\ny = df[target_col].astype(float).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.096450Z","iopub.execute_input":"2025-11-12T08:18:31.096799Z","iopub.status.idle":"2025-11-12T08:18:31.200529Z","shell.execute_reply.started":"2025-11-12T08:18:31.096766Z","shell.execute_reply":"2025-11-12T08:18:31.199571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan).astype(float)\n\nlower_q, upper_q = 0.01, 0.99\nlower_bounds = X_numeric.quantile(lower_q)\nupper_bounds = X_numeric.quantile(upper_q)\n\nX_winsorized = X_numeric.copy()\nfor col in X_winsorized.columns:\n    x = X_winsorized[col].to_numpy(dtype=float, copy=True)\n    lo = lower_bounds.get(col, np.nan)\n    hi = upper_bounds.get(col, np.nan)\n    if np.isfinite(lo):\n        x = np.where(np.isfinite(x), np.maximum(x, lo), x)\n    if np.isfinite(hi):\n        x = np.where(np.isfinite(x), np.minimum(x, hi), x)\n    X_winsorized[col] = x\n\nimputer = SimpleImputer(strategy=\"median\").fit(X_winsorized)\nX_imputed = pd.DataFrame(\n    imputer.transform(X_winsorized),\n    columns=X_winsorized.columns,\n    index=X_winsorized.index,\n)\n\nscaler = StandardScaler().fit(X_imputed)\nX_scaled = pd.DataFrame(\n    scaler.transform(X_imputed),\n    columns=X_imputed.columns,\n    index=X_imputed.index,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.201508Z","iopub.execute_input":"2025-11-12T08:18:31.201767Z","iopub.status.idle":"2025-11-12T08:18:31.294123Z","shell.execute_reply.started":"2025-11-12T08:18:31.201748Z","shell.execute_reply":"2025-11-12T08:18:31.293055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_with_target = (\n    X_scaled.assign(target = y)\n    .corr(numeric_only=True)[\"target\"]\n    .drop(\"target\")\n    .abs()\n    .sort_values(ascending=False)\n)\n\ntop_features = corr_with_target.head(min(top_k, len(corr_with_target))).index.tolist()\nbase_features = [c for c in X_scaled.columns if c not in top_features]\n\npoly_transformer = PolynomialFeatures(degree=degree, include_bias=True)\npoly_transformer.fit(X_scaled[top_features])\n\nX_poly = pd.DataFrame(\n    poly_transformer.transform(X_scaled[top_features]),\n    columns=poly_transformer.get_feature_names_out(top_features),\n    index=X_scaled.index,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.295988Z","iopub.execute_input":"2025-11-12T08:18:31.296275Z","iopub.status.idle":"2025-11-12T08:18:31.387868Z","shell.execute_reply.started":"2025-11-12T08:18:31.296255Z","shell.execute_reply":"2025-11-12T08:18:31.386939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_safe(arr, clip_limit=None):\n    arr = np.asarray(arr, dtype=float)\n    arr[~np.isfinite(arr)] = 0.0\n    if clip_limit is not None:\n        arr = np.clip(arr, -clip_limit, clip_limit)\n    return arr\n\nclip_limit = clip_val\nepsilon = eps\n\nfeature_stats = {}\nfor col in top_features:\n    values = X_imputed[col].astype(float).values\n    mean_val = float(np.nanmean(values))\n    std_val = float(np.nanstd(values))\n    feature_stats[col] = (mean_val, std_val if std_val > 0 else epsilon)\n\nfeature_list = list(top_features)\nderived_features = {}\n\nfor col in feature_list:\n    x = X_imputed[col].astype(float).values\n    mean_val, std_val = feature_stats[col]\n    std_val = std_val if std_val > 0 else epsilon\n\n    derived_features[f\"{col}__pow2\"] = make_safe(np.power(x, 2), clip_limit)\n    if degree >= 3:\n        derived_features[f\"{col}__pow3\"] = make_safe(np.power(x, 3), clip_limit)\n    if degree >= 4:\n        derived_features[f\"{col}__pow4\"] = make_safe(np.power(x, 4), clip_limit)\n\n    derived_features[f\"{col}__abs\"] = make_safe(np.abs(x), clip_limit)\n    derived_features[f\"{col}__sqrt\"] = make_safe(np.sqrt(np.clip(x, 0, None)), clip_limit)\n\n    log1p_pos = make_safe(np.log1p(np.clip(x, 0, None)), clip_limit)\n    derived_features[f\"{col}__log1p_pos\"] = log1p_pos\n    derived_features[f\"{col}__slog1p\"] = make_safe(np.sign(x) * np.log1p(np.abs(x)), clip_limit)\n    derived_features[f\"{col}__recip\"] = make_safe(1.0 / (np.where(x == 0.0, epsilon, x)), clip_limit)\n\n    z_score = make_safe((x - mean_val) / (std_val + epsilon), clip_limit)\n    derived_features[f\"{col}__z\"] = z_score\n    centered = make_safe(x - mean_val, clip_limit)\n    derived_features[f\"{col}__ctr\"]  = centered\n    derived_features[f\"{col}__ctr2\"] = make_safe(centered**2, clip_limit)\n    derived_features[f\"{col}__tanh\"] = np.tanh(x / (std_val + epsilon))\n\nfor fa, fb in combinations(feature_list, 2):\n    xa = X_imputed[fa].astype(float).values\n    xb = X_imputed[fb].astype(float).values\n\n    za = derived_features[f\"{fa}__z\"]\n    zb = derived_features[f\"{fb}__z\"]\n\n    derived_features[f\"{fa}__x__{fb}\"] = make_safe(xa * xb, clip_limit)\n    derived_features[f\"{fa}__plus__{fb}\"] = make_safe(xa + xb, clip_limit)\n    derived_features[f\"{fa}__minus__{fb}\"] = make_safe(xa - xb, clip_limit)\n    derived_features[f\"{fb}__minus__{fa}\"] = make_safe(xb - xa, clip_limit)\n    derived_features[f\"{fa}__div__{fb}\"] = make_safe(xa / (np.where(xb == 0.0, epsilon, xb)), clip_limit)\n    derived_features[f\"{fb}__div__{fa}\"] = make_safe(xb / (np.where(xa == 0.0, epsilon, xa)), clip_limit)\n\n    derived_features[f\"{fa}__z_x__{fb}__z\"] = make_safe(za * zb, clip_limit)\n    derived_features[f\"{fa}__z_div__{fb}__z_tanh\"] = np.tanh(za / (np.abs(zb) + epsilon))\n    derived_features[f\"{fa}__rel_diff__{fb}\"] = make_safe((xa - xb) / (np.abs(xa) + np.abs(xb) + epsilon), 1.0)\n    derived_features[f\"{fa}__min__{fb}\"] = np.minimum(xa, xb)\n    derived_features[f\"{fa}__max__{fb}\"] = np.maximum(xa, xb)\n\n    la = derived_features[f\"{fa}__log1p_pos\"]\n    lb = derived_features[f\"{fb}__log1p_pos\"]\n    derived_features[f\"{fa}__log_x__{fb}__log\"] = make_safe(la * lb, clip_limit)\n\n    pa = np.clip(xa, 0, None)\n    pb = np.clip(xb, 0, None)\n    derived_features[f\"{fa}__gmean__{fb}\"] = make_safe(np.sqrt(pa * pb), clip_limit)\n    derived_features[f\"{fa}__hmean__{fb}\"] = make_safe(2.0 * pa * pb / (pa + pb + epsilon), clip_limit)\n\n    derived_features[f\"{fa}2__x__{fb}__z\"] = make_safe((za * za) * zb, clip_limit)\n    derived_features[f\"{fa}__x__{fb}2__z\"] = make_safe(za * (zb * zb), clip_limit)\n\nderived_df = pd.DataFrame(derived_features, index=X_imputed.index)\nderived_cols = derived_df.columns.tolist()\n\nderived_scaler = StandardScaler().fit(derived_df)\nderived_scaled = pd.DataFrame(\n    derived_scaler.transform(derived_df),\n    columns=derived_cols,\n    index=derived_df.index\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.388863Z","iopub.execute_input":"2025-11-12T08:18:31.389121Z","iopub.status.idle":"2025-11-12T08:18:31.604213Z","shell.execute_reply.started":"2025-11-12T08:18:31.389101Z","shell.execute_reply":"2025-11-12T08:18:31.603421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ohe_train = pd.get_dummies(X_categorical, drop_first=True)\nohe_cols = ohe_train.columns.tolist()\n\ndesign_X = pd.concat(\n    [\n        X_scaled[base_features].reset_index(drop=True),\n        X_poly.reset_index(drop=True),\n        derived_scaled.reset_index(drop=True),\n        ohe_train.reset_index(drop=True),\n    ],\n    axis = 1\n)\n\nridge = Ridge(alpha=alpha, random_state=42).fit(design_X, y)\ny_pred = ridge.predict(design_X)\ntrain_mse = mean_squared_error(y, y_pred)\nprint(f\"train MSE: {train_mse:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:31.605095Z","iopub.execute_input":"2025-11-12T08:18:31.605408Z","iopub.status.idle":"2025-11-12T08:18:32.387285Z","shell.execute_reply.started":"2025-11-12T08:18:31.605383Z","shell.execute_reply":"2025-11-12T08:18:32.386336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(test_csv)\n\nX_numeric_test = df_test.select_dtypes(include=[np.number]).reindex(columns=X_numeric.columns)\nX_categorical_test = df_test[cat_cols].copy()\n\nX_numeric_test = X_numeric_test.replace([np.inf, -np.inf], np.nan).astype(float)\n\nX_winsorized_test = X_numeric_test.copy()\nfor col in X_winsorized_test.columns:\n    x = X_winsorized_test[col].to_numpy(dtype=float, copy=True)\n    lo = lower_bounds.get(col, np.nan)\n    hi = upper_bounds.get(col, np.nan)\n    if np.isfinite(lo):\n        x = np.where(np.isfinite(x), np.maximum(x, lo), x)\n    if np.isfinite(hi):\n        x = np.where(np.isfinite(x), np.minimum(x, hi), x)\n    X_winsorized_test[col] = x\n\nX_imputed_test = pd.DataFrame(\n    imputer.transform(X_winsorized_test),\n    columns = X_winsorized_test.columns,\n    index = X_winsorized_test.index,\n)\n\nX_scaled_test = pd.DataFrame(\n    scaler.transform(X_imputed_test),\n    columns = X_imputed_test.columns,\n    index = X_imputed_test.index,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:32.387952Z","iopub.execute_input":"2025-11-12T08:18:32.388241Z","iopub.status.idle":"2025-11-12T08:18:32.452411Z","shell.execute_reply.started":"2025-11-12T08:18:32.388216Z","shell.execute_reply":"2025-11-12T08:18:32.451456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_poly_test = pd.DataFrame(\n    poly_transformer.transform(X_scaled_test[top_features]),\n    columns=poly_transformer.get_feature_names_out(top_features),\n    index=X_scaled_test.index,\n)\n\nfeature_list = list(top_features)\nderived_test = {}\n\nfor col in feature_list:\n    x = X_imputed_test[col].astype(float).values\n    mean_val, std_val = feature_stats[col]\n    std_val = std_val if std_val > 0 else epsilon\n\n    derived_test[f\"{col}__pow2\"] = make_safe(np.power(x, 2), clip_limit)\n    if degree >= 3:\n        derived_test[f\"{col}__pow3\"] = make_safe(np.power(x, 3), clip_limit)\n    if degree >= 4:\n        derived_test[f\"{col}__pow4\"] = make_safe(np.power(x, 4), clip_limit)\n\n    derived_test[f\"{col}__abs\"] = make_safe(np.abs(x), clip_limit)\n    derived_test[f\"{col}__sqrt\"] = make_safe(np.sqrt(np.clip(x, 0, None)), clip_limit)\n\n    log1p_pos = make_safe(np.log1p(np.clip(x, 0, None)), clip_limit)\n    derived_test[f\"{col}__log1p_pos\"] = log1p_pos\n    derived_test[f\"{col}__slog1p\"] = make_safe(np.sign(x) * np.log1p(np.abs(x)), clip_limit)\n    derived_test[f\"{col}__recip\"] = make_safe(1.0 / (np.where(x == 0.0, epsilon, x)), clip_limit)\n\n    z_score = make_safe((x - mean_val) / (std_val + epsilon), clip_limit)\n    derived_test[f\"{col}__z\"] = z_score\n    centered = make_safe(x - mean_val, clip_limit)\n    derived_test[f\"{col}__ctr\"] = centered\n    derived_test[f\"{col}__ctr2\"] = make_safe(centered**2, clip_limit)\n    derived_test[f\"{col}__tanh\"] = np.tanh(x / (std_val + epsilon))\n\nfor fa, fb in combinations(feature_list, 2):\n    xa = X_imputed_test[fa].astype(float).values\n    xb = X_imputed_test[fb].astype(float).values\n\n    za = derived_test[f\"{fa}__z\"]\n    zb = derived_test[f\"{fb}__z\"]\n\n    derived_test[f\"{fa}__x__{fb}\"] = make_safe(xa * xb, clip_limit)\n    derived_test[f\"{fa}__plus__{fb}\"] = make_safe(xa + xb, clip_limit)\n    derived_test[f\"{fa}__minus__{fb}\"] = make_safe(xa - xb, clip_limit)\n    derived_test[f\"{fb}__minus__{fa}\"] = make_safe(xb - xa, clip_limit)\n    derived_test[f\"{fa}__div__{fb}\"] = make_safe(xa / (np.where(xb == 0.0, epsilon, xb)), clip_limit)\n    derived_test[f\"{fb}__div__{fa}\"] = make_safe(xb / (np.where(xa == 0.0, epsilon, xa)), clip_limit)\n\n    derived_test[f\"{fa}__z_x__{fb}__z\"] = make_safe(za * zb, clip_limit)\n    derived_test[f\"{fa}__z_div__{fb}__z_tanh\"] = np.tanh(za / (np.abs(zb) + epsilon))\n    derived_test[f\"{fa}__rel_diff__{fb}\"] = make_safe((xa - xb) / (np.abs(xa) + np.abs(xb) + epsilon), 1.0)\n    derived_test[f\"{fa}__min__{fb}\"] = np.minimum(xa, xb)\n    derived_test[f\"{fa}__max__{fb}\"] = np.maximum(xa, xb)\n\n    la = derived_test[f\"{fa}__log1p_pos\"]\n    lb = derived_test[f\"{fb}__log1p_pos\"]\n    derived_test[f\"{fa}__log_x__{fb}__log\"] = make_safe(la * lb, clip_limit)\n\n    pa = np.clip(xa, 0, None)\n    pb = np.clip(xb, 0, None)\n    derived_test[f\"{fa}__gmean__{fb}\"] = make_safe(np.sqrt(pa * pb), clip_limit)\n    derived_test[f\"{fa}__hmean__{fb}\"] = make_safe(2.0 * pa * pb / (pa + pb + epsilon), clip_limit)\n\n    derived_test[f\"{fa}2__x__{fb}__z\"] = make_safe((za * za) * zb, clip_limit)\n    derived_test[f\"{fa}__x__{fb}2__z\"] = make_safe(za * (zb * zb), clip_limit)\n\nderived_test_df = pd.DataFrame(derived_test, index = X_imputed_test.index).reindex(columns = derived_cols, fill_value = 0.0)\nderived_test_scaled = pd.DataFrame(\n    derived_scaler.transform(derived_test_df),\n    columns = derived_cols,\n    index = derived_test_df.index,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:32.453540Z","iopub.execute_input":"2025-11-12T08:18:32.453851Z","iopub.status.idle":"2025-11-12T08:18:32.577757Z","shell.execute_reply.started":"2025-11-12T08:18:32.453820Z","shell.execute_reply":"2025-11-12T08:18:32.576830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ohe_test = pd.get_dummies(X_categorical_test, drop_first = True).reindex(columns = ohe_cols, fill_value = 0)\n\ndesign_X_test = pd.concat(\n    [\n        X_scaled_test[base_features].reset_index(drop = True),\n        X_poly_test.reset_index(drop = True),\n        derived_test_scaled.reset_index(drop = True),\n        ohe_test.reset_index(drop = True),\n    ],\n    axis=1\n)\n\ny_pred_test = ridge.predict(design_X_test)\nsubmission = pd.DataFrame({\"prediction\": y_pred_test})\n\nif \"ID\" in df_test.columns:\n    submission.insert(0, \"ID\", df_test[\"ID\"].values)\n\nsubmission.to_csv(\"predictions.csv\", index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T08:18:32.578602Z","iopub.execute_input":"2025-11-12T08:18:32.578839Z","iopub.status.idle":"2025-11-12T08:18:32.691986Z","shell.execute_reply.started":"2025-11-12T08:18:32.578820Z","shell.execute_reply":"2025-11-12T08:18:32.690998Z"}},"outputs":[],"execution_count":null}]}